L’entropie est un concept qui transcende les disciplines scientifiques : on le retrouve en physique pour caract ́eriser le d ́esordre d’un syst`eme, en th ́eorie de l’information pour caract ́eriser la quantit ́e d’information, en math ́ematiques pour l’incertitude. Rien qu’en informatique ses applications sont fondamentales et tr`es diverses : en compression, en aide `a la d ́ecision, dans des probl`emes de s ́eparation de sources (en signal par exemple), en algorithmique pour l’approximation de probl`emes NP-complet. La notion d’entropie a  ́et ́e principalement formalis ́ee en informatique par Shannon vers 1948. Le probl`eme auquel il s’attaquait  ́etait de r ́eussir a` quantifier la notion d’information contenue dans un message  ́emis par une source vers un r ́ecepteur (pour r ́epondre a` des questions de bruitage), ce qui a contribu ́e a` la naissance a` la th ́eorie de l’information.Supposons que l’on joue a` deviner un nombre x∗ dans N = [0,100] : a` chaque tour le joueur peut demander si le nombre a` deviner est plus grand ou plus petit qu’un nombre x. Une strat ́egie naturelle consiste `a d ́ecouper l’ensemble en deux parties  ́egales, et ceci pour une raison simple : soit Ax et Bx la partition op ́er ́ee par x, Ax contenant les  ́el ́ements inf ́erieurs a` x et B ceux sup ́erieurs. La probabilit ́e de trouver x lorsque l’on sait x∗ est dans Ax ou Bx est identique, 2/|N|. Si le d ́ecoupage n’ ́etait pas  ́egal, en fonction de la r ́eponse, notre probabilit ́e aurait pu ˆetre meilleur ou pire. Une strat ́egie conservative nous fait pr ́ef ́erer naturellement le partitionnement  ́equilibr ́e, en consid ́erant que sans autre information suppl ́ementaire, il est pr ́ef ́erable de garder des probabilit ́es optimales quelque soit la r ́eponse a` la question.En communiquant une unit ́e d’information, on a divis ́e par 2 le nombre de possibilit ́es. Selon cette m ́ethode, pour sp ́ecifier compl`etement x∗, on aurait besoin de log2 |N| unit ́es d’information. Sp ́ecifier un  ́el ́ement dans Ax (sachant qu’il appartient `a Ax) revient `a transmettre selon le mˆeme raisonnement log2 |Ax| information. Ainsi, lorsqu’on transmet l’information qu’un  ́el ́ement appartient `a |Ax, on transmet en fait log |N | − log |A | = log |N | unit ́es d’information.2 2x 2|Ax|Supposons maintenant que le joueur adverse est notre meilleur ami d’enfance et que l’on sait qu’il y a une chance sur deux pour que le nombre soit entre 0 et 10. Devrait-on faire le mˆeme choix ? Si on posecomme premi`ere question x = 10 ?, alors la probabilit ́e de trouver la r ́eponse est 0.5 ∗ 0.1 + 0.5 ∗ 0.9 =0.055, plus  ́elev ́e que pr ́ec ́edemment.On complique le jeu : vous devez devinez deux nombres cette fois, toujours face `a votre meilleur amid’enfance. Si un tricheur vous communique l’un des deux nombres, pr ́ef ́erez vous que ce soit un nombrede A10 entre 0 et 10 ou de B10 entre 10 et 100 ? La part d’information que l’on vous communique surles deux nombres a` deviner est toujours la mˆeme pour n’importe quel nombre de A10, de mˆeme pourn’importe quel nombre de B10. Cependant, comme il vous faudrait plus de questions pour sp ́ecifierun nombre de B10 (ou vous avez une plus faible chance de trouver au hasard un nombre de B10), letricheur vous aide beaucoup plus en vous communiquant un nombre de B10 que de A10 qui auraitsinon une probabilit ́e de 0.5|B10| d’ˆetre trouv ́e. En reprenant le calcul ci-dessus, l’information d’un |N| ́el ́ement x de B10 est log2 0.5 |N | = − log2 P (x), P  ́etant la distribution de probabilit ́e sur les choixde mon ami.L’entropie S peut ˆetre vu comme l’information moyenne obtenue en tirant un  ́el ́ement d’une distribu- tion, soit pour une variable al ́eatoire X de n modalit ́es : S(X) = −  ni=1 pi log2 pi. Plus la distribution est proche de la distribution uniforme, plus l’entropie est grande (maximum en − log2 n). Plus un ́el ́ement de la distribution est pr ́epond ́erant, plus l’entropie est basse. L’entropie est ainsi une mesurede dispersion de la loi de probabilit ́e.Pour deux distributions P et Q, on d ́efinit l’entropie relative S(P ||Q) =   P (x) log P (x) qui peut x 2 Q(x)ˆetre vu comme une sorte de distance entre les deux distributions (distance non sym ́etrique).Pr ́eliminaires Codez une fonction entropie(l) qui permet de calculer l’entropie de la distribution repr ́esent ́ee par la liste l = [p1, ..., pn]. Codez la fonction divergence(p,q) qui code l’entropie relative (appel ́ee aussi divergence) entre la distribution p et q.Tracez le graphique de l’entropie pour une variable al ́eatoire binaire, pouvant prendre 2 valeurs uni- quement, l’une avec une probabilit ́e p l’autre 1 − p, en fonction de p.2 Entropie et classification de texteSoit X une variable al ́eatoire a` valeur dans l’ensemble des caract`eres d’un alphabet A. Un texte de longueur n est la r ́ealisation des n variables al ́eatoires X1X2 · · · Xn (ce qu’on appelle un processus s ́equentiel). X1 d ́enote le premier caract`ere, X2 le deuxi`eme et ainsi de suite. On peut ainsi d ́efinir la distribution Pn sur les textes de longueur n. Si les textes d’une langue  ́etaient compos ́es de toutes les combinaisons possibles de caract`ere de la langue, Pn serait uniforme. Ce n’est bien suˆr pas le cas. L’objectif de cette partie est d’ ́etudier l’information contenue dans ses distributions sur les langues.2.1 Entropie d’une langueOn d ́efinit l’entropie d’un processus s ́equentiel comme la limite de S(X1X2···Xn) quand n tend vers nl’infini :— pour n = 1, S(X1) = − x∈A p(X1 = x)logp(X1 = x),— pour n = 2, S(X1X2) = − xy∈A p(X1 = x,X2 = y)logp(X1 = x,X2 = y) (la probabilit ́e descombinaisons de deux lettres) et ainsi de suite.Il est bien suˆr impossible de calculer explicitement cette entropie, on peut cependant l’estimer pour quelques valeurs petites de n (et par d’autres moyens plus avanc ́es). Pour n = 1, il nous faut estimer la probabilit ́e d’apparition de chaque caract`ere ; pour n = 2 celle d’apparition de couple de caract`eres,. . .. Codez les fonctions n ́ecessaires a` ce calcul, en particulier la fonction count ngrams(text,n) qui `a par- tir d’un texte text et d’un entier n, rend le dictionnaire ou` chaque cl ́e est une s ́equence de n caract`eres et la valeur associ ́ee le nombre de fois ou` elle apparaˆıt dans le texte (une suite de n caract`eres s’ap- pellent  ́egalement un n-gram). Vous pourrez utiliser l’objet Counter() du module collections pour compter le nombre d’occurences. Par simplification, vous pouvez ignorer les accents et les caract`eres sp ́eciaux.Indications Pour traiter les accents et autres caract`eres sp ́eciaux, prenez soin de sauvegarder vos fichiers textes en utf8, vous pouvez ensuite utiliser ce code pour substituer la lettre correspondante aux caract`eres accentu ́es.1 import codecs2 import unicodedata3 with codecs.open(fn,encoding="utf-8") as f:4 s=f . read ()5 s=unicodedata.normalize("NFKD",s).encode("ascii","ignore")Vous pouvez  ́egalement construire une correspondance (mapping) entre caract`eres `a subsituer et leur substitution en utilisant string.translate :1 import string2 table=string . maketrans("abc" ,"bcd") #table des correspondances3#a −> b, b−>c , c−>d4 s=string.translate(s,table ,"!") # dernier parametre : caracteres a supprimerEnfin le code suivant enl`eve les caract`eres d’espacement cons ́ecutifs et les remplacent pas un unique espace :1 import re2 re = re.sub("\s","␣",re.sub("\s(?=\s)","␣",s))Exp ́eriences— Testez vos fonctions sur des textes d’une mˆeme langue (par exemple `a partir du site www. gutenberg.org). Calculez l’entropie pour les n qui ne posent pas de probl`eme m ́emoire. Que remarquez vous selon les langues et les valeurs de n ?— Soit T un texte et Pl la distribution de la langue l pour un certain nombre de langues. En utilisant l’entropie relative S(T||Pl), on peut tenter de classifier le texte parmi une des langues. Conduisez des exp ́eriences en ce sens. Est-ce une m ́ethode fiable? Testez plusieurs longueurs de texte. Peut-on classer un mot ?— Tracez les distributions Snl sous forme d’histogramme. Cela vous semble-t-il plus informatif ? 2.2 Classification bay ́esiennePlutoˆt que de s’int ́eresser au caract`ere global des distributions, nous allons utiliser dans cette partie les probabilit ́es de chaque sous-s ́equence pour calculer la probabilit ́e qu’une s ́equence appartient `a une langue. Nous noterons dans la suite w = w1w2w3 · · · wn un texte sur l’alphabet A dont les lettres sont w1, w2, w3 ···.Nous voulons calculer : l∗ = argmaxl∈L p(l|w) ou` L est l’ensemble des langues que notre syst`emeconnaˆıt, w le texte consid ́er ́e et l∗ est le r ́esultat de la pr ́ediction. La probabilit ́e p(l|w) peut ˆetred ́etermin ́ee graˆce `a la formule de Bayes : p(l|w) = p(w|l)·p(l) p(w)A quoi correspondent les diff ́erents termes de cette relation ? Peuvent-ils ˆetre tous calcul ́es ? En par- ticulier, est-il n ́ecessaire de d ́eterminer la probabilit ́e p(w) ? Quelle hypoth`ese faut-il faire pour p(l) ?Estimation du mod`ele Pour estimer p(w|l), nous allons proc ́eder `a une simplification, en consid ́erant que toutes les lettres sont ind ́ependantes : P(X1 = w1,X2 = w2,...Xn = wn) = P(X1 = w1)P(X2 = w2) . . . P (Xn = wn). En quoi est-ce faux ? Comment se comporte cette probabilit ́e quand n augmente ? Cette m ́ethode fonctionne-t-elle mieux pour la d ́etection de langue d’un mot ou d’un texte ? Comment g ́en ́eraliser `a la d ́etection d’un texte ?Exp ́eriences Comparez les r ́esultats de cette m ́ethode par rapport `a la m ́ethode entropique. Faites varier le nombre de texte qui servent `a l’estimation des param`etres, comment se comportent les r ́esultats ?Am ́elioration On peut am ́eliorer le mod`ele en supposant une d ́ependance d’ordre 1 entre les lettres, c’est-a`-dire que Xi,Xi+1 sont d ́ependants. Dans ce cas, on aP (X) = P (X1, ..., Xn) = P (Xn|X1...Xn−1)P (X1...Xn−1) = P (Xn|Xn−1)P (X1...Xn−1). En continuant le d ́eveloppement, on obtient P (X) = P (Xn|Xn−1)P (Xn−1|Xn−2)...P (X2|X1)P (X1).Codez cette am ́elioration. Qu’observez vous exp ́erimentalement? Est-il possible de g ́en ́eraliser `a des d ́ependances d’ordre 2,3, etc ?G ́en ́eration de texte Comment utiliser les r ́esultats ci-dessus afin d’engendrer un texte d’une taille n donn ́ee en fonction du mod`ele du langage ? Codez cette fonction et exp ́erimentez.2.3 Codage de HuffmanLe codage de Huffman est un algorithme de compression de donn ́ees. Soit A l’alphabet de repr ́esentation des donn ́ees et une donn ́ee w = w1..wn. Pour d ́ecrire cette donn ́ee, on doit repr ́esenter informatique- ment chaque lettre de l’alphabet par un code et la longueur de description de la donn ́ee (la longueur de codage) est l(w) =  l(wi). Si on consid`ere l’alphabet de 26 lettres, on a besoin de log2(26) bits pour d ́ecrire chaque lettre, soit 5 bits (cf introduction). Dans ce cas, la longueur de codage de chaque mot ne d ́epend que du nombre de caract`ere, l(w) = nlog2(26). L’esp ́erance sur tous les mots de la langue L  ́etudi ́ee nous donne la longueur moyenne de codage d’un mot de la langue.Lorsque les lettres ne sont pas  ́equiprobables, ce codage n’est pas optimale : on souhaiterait utiliser des codes plus longs pour d ́ecrire des lettres tr`es rares, des codes tr`es courts pour des lettres tr`es fr ́equentes afin de minimiser l’esp ́erance. En effet, si on consid`ere chaque lettre ind ́ependante l’une de l’autre, l’esp ́erance de la longueur de codage d’un mot de longueur m pour une langue L est EL(l) = 1   l(w) =     l(wi) = m  l(wi)p(wi) |L| w∈L,|w|=m w∈L,|w|=m wi ∈w wi ∈ALe codage de Huffman est un algorithme qui garantie l’optimalit ́e du code obtenu (en consid ́erant les lettres ind ́ependantes). Le but est de construire un arbre binaire, tel que une lettre est associ ́ee de fa ̧con bijective a` une feuille de l’arbre et que le chemin de la racine a` la feuille indique le codage de la lettre.Pour trouver le codage d’une lettre, on parcourt l’arbre de la racine `a la feuille concern ́ee, en notant successivement 0 si la branche gauche du nœud a  ́et ́e choisie, 1 si c’est la branche droite. A l’issue du parcourt, le chemin est d ́ecrit par une suite de {0, 1} selon les embranchements choisis. La lettre peut ˆetre remplac ́ee par ce code, d ́ecodable de mani`ere unique (en parcourant l’arbre selon la succession de 0 et de 1). Soit c(wi) le codage de la lettre wi, la longueur de la donn ́ee compress ́ee est alors l(c(w)) =   l(c(wi)) : plus la feuille correspondant a` la lettre est profonde, plus la longueur du codage est grande (quelle lien entre les deux ?).L’algorithme propos ́e par Huffman pour aboutir a` un arbre de codage optimal (tel que l’esp ́erance de la longueur de l’encodage soit minimale) consiste a` construire de mani`ere it ́erative l’arbre `a partir de l’ensemble des feuilles.— T est initialis ́e comme un ensemble d’arbres ti de profondeur 0, chacun correspondant `a une lettre de l’alphabet. On d ́enotera pti le poids associ ́e a` l’arbre ti : dans le cas d’une feuille il s’agit de la probabilit ́e d’apparition de la lettre correspondante.— a` chaque  ́etape, on choisit les deux arbres ti et tj de T tels que les poids pti et ptj des deux arbres soient minimales. Les deux arbres sont retir ́es de la liste, un nouvel arbre tk est ajout ́e dans la liste tel que le fils gauche de la racine de tk soit ti et le fils droit tj. Le poids du nouvel arbre est ptk = pti + ptj : il correspond a` la probabilit ́e d’apparition d’une des lettres cod ́es par le sous-arbre.— On it`ere le processus tant qu’il reste plus d’un arbre dans T. L’arbre final est l’arbre de codage A l’aide de vos fonctions pr ́ec ́edentes, codez les fonctions :— codage(liste) qui prend une liste de couples (wi,pi) (la lettre et sa probabilit ́e d’apparition) et renvoie l’arbre de codage optimal ;— encode(s,code) qui permet d’encoder le mot s avec l’arbre de codage code ;— decode(s,code) qui permet de d ́ecoder le mot s avec l’arbre de codage code.Exp ́erimentez sur la longueur des codes obtenus dans le cas d’un langage al ́eatoire, dans le cas des langues que vous avez exp ́erimentez ci-dessus.3 Mastermind (bonus)Le mastermind est un jeu de logique entre 2 joueurs : le premier joueur choisit un code de longueur n sur un alphabet de taille c (g ́en ́eralement des couleurs) : code = c1...cn, le deuxi`eme joueur a pour but de trouver le code en posant successivement des questions qui sont elles-mˆemes un code possible; il re ̧coit une r ́eponse lui indiquant d’une part le nombre de couleurs (lettres) `a la bonne place et d’autre part le nombre de couleurs bonnes mais pas a` la bonne place. Chaque r ́eponse permet d’ ́eliminer une partie des codes. Le joueur aboutit `a une solution lorsque l’ensemble des solutions possibles est r ́eduit `a un  ́el ́ement. Ainsi le nombre d’ ́etapes n ́ecessaires `a la r ́esolution du code d ́epend du nombre de solutions possibles a` chaque question. On se propose dans cette partie d’ ́etudier quelques strat ́egies de r ́esolutions. Soit Et l’ensemble des solutions au t tours.— strat ́egie al ́eatoire : au d ́ebut l’ensemble des solutions possibles est organis ́e al ́eatoirement, la strat ́egie consiste ensuite `a poser comme question une solution possible et d’ ́eliminer les solutions non compatibles avec la r ́eponse obtenue (`a tirer al ́eatoirement dans Et).— strat ́egie pessimiste : on privil ́egie la question qui permet de minimiser la taille maximale de Et+1 dans le cas de la pire r ́eponse obtenue.— strat ́egie entropique : on privil ́egie la question qui permet d’ ́equilibrer au mieux les ensembles de solutions Et+1 possibles apr`es avoir obtenue la r ́eponse. Ceci revient a` maximiser une entropie (laquelle ?).Le code pour le jeu est fourni ci-dessous. Impl ́ementez les strat ́egies (et d’autres si vous avez des id ́ees) et calculez l’esp ́erance et la variance du nombre de coup n ́ecessaire `a la r ́esolution en fonction de la strat ́egie.definit     (self ,coul=6,n=4): self.coul, self.n = coul, n self.step = 0self.reset()  defdef reset(self):self.code = [ random.randint(1,self.coul) for x in range(self.n)] self.step = 0def play(self ,coup): self.step +=1return evalue(coup, self .code)evalue(question ,code):black = sum([1 for c,d in zip(question ,code) if c == d])white = sum([min(question.count(i+1),code.count(i+1)) for i in range(max(question+code)) ]) − black return black , whitejeu = mastermind() jeu.play([1,2,1,2])